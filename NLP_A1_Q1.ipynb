{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YM4KNU0rvXzx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8997b58b-2986-49db-fba3-5afdc048a09e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word after merging: ('t', 'e', 's', 't$')\n",
            "word after merging: ('t', 'es', 't$')\n",
            "word after merging: ('s', 'a', 'm', 'p', 'l', 'e$')\n",
            "word after merging: ('s', 'a', 'm', 'p', 'le$')\n",
            "word after merging: ('l', 'o', 'w', 'e', 's', 't$')\n",
            "word after merging: ('l', 'ow', 'e', 's', 't$')\n",
            "word after merging: ('l', 'ow', 'es', 't$')\n",
            "word after merging: ('l', 'o', 'w', 'in', 'g', '$')\n",
            "word after merging: ('l', 'o', 'w', 'ing', '$')\n",
            "word after merging: ('l', 'o', 'w', 'ing$')\n",
            "word after merging: ('l', 'ow', 'ing$')\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import collections\n",
        "\n",
        "class BytePairEncoding:\n",
        "    def __init__(self):\n",
        "        self.bpe_codes = {}\n",
        "        self.bpe_codes_reverse = {}\n",
        "        self.vocabulary = set()\n",
        "\n",
        "    def create_vocab(self, sentence):\n",
        "        words = sentence.split()\n",
        "        vocab = {}\n",
        "\n",
        "        for word in words:\n",
        "            characters = list(word)\n",
        "            characters.append('$')\n",
        "            vocab[' '.join(characters)] = vocab.get(' '.join(characters), 0) + 1\n",
        "\n",
        "        return vocab\n",
        "\n",
        "    def get_stats(self, vocab):\n",
        "        pairs = collections.defaultdict(int)\n",
        "        for word, freq in vocab.items():\n",
        "            symbols = word.split()\n",
        "            for i in range(len(symbols)-1):\n",
        "                pairs[symbols[i], symbols[i+1]] += freq\n",
        "        return pairs\n",
        "\n",
        "    def merge_vocab(self, pair, v_in):\n",
        "        v_out = {}\n",
        "        bigram = re.escape(' '.join(pair))\n",
        "        p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
        "        for word in v_in:\n",
        "            w_out = p.sub(''.join(pair), word)\n",
        "            v_out[w_out] = v_in[word]\n",
        "        return v_out\n",
        "\n",
        "    def learn_vocabulary(self, corpus, num_merges):\n",
        "        train_data = self.create_vocab(corpus)\n",
        "\n",
        "        for i in range(num_merges):\n",
        "            pairs = self.get_stats(train_data)\n",
        "            best = max(pairs, key=pairs.get)\n",
        "            train_data = self.merge_vocab(best, train_data)\n",
        "\n",
        "            self.bpe_codes[best] = i\n",
        "            self.bpe_codes_reverse[best[0] + best[1]] = best\n",
        "\n",
        "        self.vocabulary = set(train_data.keys())\n",
        "\n",
        "    def get_pairs(self, word):\n",
        "        pairs = set()\n",
        "        prev_char = word[0]\n",
        "        for char in word[1:]:\n",
        "            pairs.add((prev_char, char))\n",
        "            prev_char = char\n",
        "        return pairs\n",
        "\n",
        "    def tokenize(self, sentence):\n",
        "        words = sentence.split()\n",
        "        tokenized_sentence = []\n",
        "\n",
        "        for word in words:\n",
        "            word_tokens = self._tokenize_word(word)\n",
        "            tokenized_sentence.extend(word_tokens)\n",
        "\n",
        "        return tokenized_sentence\n",
        "\n",
        "    def _tokenize_word(self, orig):\n",
        "        word = tuple(orig) + ('$',)\n",
        "\n",
        "        pairs = self.get_pairs(word)\n",
        "\n",
        "        if not pairs:\n",
        "            return [orig]\n",
        "\n",
        "        while True:\n",
        "            bigram = min(pairs, key=lambda pair: self.bpe_codes.get(pair, float('inf')))\n",
        "            if bigram not in self.bpe_codes:\n",
        "                break\n",
        "            first, second = bigram\n",
        "            new_word = []\n",
        "            i = 0\n",
        "            while i < len(word):\n",
        "                try:\n",
        "                    j = word.index(first, i)\n",
        "                    new_word.extend(word[i:j])\n",
        "                    i = j\n",
        "                except:\n",
        "                    new_word.extend(word[i:])\n",
        "                    break\n",
        "\n",
        "                if word[i] == first and i < len(word) - 1 and word[i+1] == second:\n",
        "                    new_word.append(first + second)\n",
        "                    i += 2\n",
        "                else:\n",
        "                    new_word.append(word[i])\n",
        "                    i += 1\n",
        "            new_word = tuple(new_word)\n",
        "            word = new_word\n",
        "            print(\"word after merging: {}\".format(word))\n",
        "            if len(word) == 1:\n",
        "                break\n",
        "            else:\n",
        "                pairs = self.get_pairs(word)\n",
        "\n",
        "        # if word[-1] == '$':\n",
        "        #     word = word[:-1]\n",
        "        # elif word[-1].endswith('$'):\n",
        "        #     word = word[:-1] + (word[-1].replace('$', ''),)\n",
        "\n",
        "        return list(word)\n",
        "\n",
        "    def save_vocabulary(self, file_path=\"vocabulary.txt\"):\n",
        "        with open(file_path, 'w') as file:\n",
        "            for token in self.vocabulary:\n",
        "                file.write(token + '\\n')\n",
        "\n",
        "    def save_merge_rules(self, file_path=\"merge_rules.txt\"):\n",
        "        with open(file_path, 'w') as file:\n",
        "            for rule, merge_id in self.bpe_codes.items():\n",
        "                file.write(f\"{rule[0]},{rule[1]}: {merge_id}\\n\")\n",
        "\n",
        "    def tokenize_and_save(self, test_samples, file_path=\"tokenized_samples.txt\"):\n",
        "        with open(file_path, 'w') as file:\n",
        "            for sample in test_samples:\n",
        "                tokens = self.tokenize(sample)\n",
        "                file.write('\\n'.join(tokens) + '\\n')\n",
        "\n",
        "# Example usage:\n",
        "bpe = BytePairEncoding()\n",
        "file_path = '/content/corpus.txt'\n",
        "corpus=\"\"\n",
        "try:\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            corpus = corpus +\" \"+ line.strip()\n",
        "except FileNotFoundError:\n",
        "    print(f\"File '{file_path}' not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "num_merges = 100\n",
        "bpe.learn_vocabulary(corpus, num_merges)\n",
        "\n",
        "# Save Vocabulary\n",
        "bpe.save_vocabulary()\n",
        "\n",
        "# Save Merge Rules\n",
        "bpe.save_merge_rules()\n",
        "\n",
        "test_samples = [\"test sample 1 lowest lowing\"]\n",
        "bpe.tokenize_and_save(test_samples)"
      ]
    }
  ]
}